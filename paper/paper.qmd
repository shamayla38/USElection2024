---
title: "Forecasting the 2024 U.S. Presidential Election: An Analysis of Battleground States"
subtitle: "Trump Favored Nationally, While Harris Leads in 4 of 7 Battleground States"
author: 
  - Shamayla Durrin
thanks: "Code and data are available at: [https://github.com/krishnak30/US_elections](https://github.com/krishnak30/US_elections)."
date: today
date-format: long
abstract: >
  This study employs a poll-of-polls approach to predict support for Kamala Harris and Donald Trump in the 2024 U.S. Presidential Election, focusing on key battleground states to forecast the likely winner of the electoral college. By aggregating multiple polls and applying a weighted linear regression model with predictors like pollster reliability, sample size, state, and recency, we estimate higher national support for Trump than Harris. Our analysis also shows a close competition in battleground states, with Trump holding a slight lead in Arizona, Georgia, and North Carolina, while Harris leads in Michigan, Nevada, Pennsylvania, and Wisconsin. These findings show the value of aggregating polls over relying on individual surveys, offering a robust forecast of electoral outcomes.
format: pdf
number-sections: true
bibliography: references.bib
---

```{r}
#| include: false
#| warning: false
#| message: false

#### Workspace set up ####
library(psych)
library(tidyverse)
library(arrow)
library(here)
library(kableExtra)
library(plotly)
library(RColorBrewer)
library(modelsummary)
library(broom)
library(sf)
library(patchwork)
library(ggfortify)

# Load data
analysis_data <- read_parquet(here("data/02-analysis_data/analysis_data.parquet"))
us_states <- st_read(here("data/03-mapping_data/US"))
```

# Introduction

While individual polls provide snapshots of public opinion, they are often subject to biases and methodological differences. In this paper, we aim to forecast voter support for Kamala Harris and Donald Trump by using data from multiple polling sources, reducing individual poll biases, and improving overall prediction accuracy. Our analysis estimates the levels of support for each candidate nationally while focusing on swing states, which are likely to be decisive in determining the Electoral College outcome.

The estimand of this study is the level of voter support for each candidate, Kamala Harris and Donald Trump, as reported across multiple polls. To estimate this, we developed a regression model incorporating variables such as pollster, sample size, state, and recency of the poll, with an emphasis on accurately capturing state-level dynamics. Our findings indicate that, on a national level, support between Kamala Harris and Donald Trump is closely balanced. Harris’s estimated national support is around 48%, while Trump’s support is slightly higher, around 49%. We found that Trump holds a lead in battleground states like Arizona and Georgia, with a support margin of over 1%. In contrast, Harris shows small leads in Michigan, Nevada, Pennsylvania, and Wisconsin, with her support margin in Wisconsin over 2%, making it her strongest battleground. North Carolina is one of the tightest races, with Trump leading by only 0.26%. This analysis is useful for political strategists, media analysts, and the general public by offering a view of the electoral landscape.

This paper is organized as follows: In Section @sec-data, we show summary statistics, plot distributions of key variables, and examine relationships between variables. In Section @sec-model, we discuss our forecasting approach, model selection, justification for the chosen model, and the mechanism of deriving poll weights based on pollster reliability, ultimately presenting our predictions. In Section @sec-discussion, we address the broader implications of our findings, acknowledge limitations, and suggest directions for future work. [Appendix -@sec-data-model] contains details of the data cleaning process and model diagnostics.

# Data {#sec-data}

## Variables of Interest

### Summary Statistics of Key Variables and Measurement Method

We measure voters' intentions by surveying a sample of individuals at various points leading up to the election, asking about their current opinions on candidates, issues, or likelihood to vote. Each response is recorded with a timestamp, allowing us to analyze changes over time (recency) and connect shifting opinions to projected behaviors. We then aggregate and analyze these responses to predict patterns, identifying trends that could indicate likely voting actions on Election Day.

For this analysis, a subset of variables was selected from the raw data set. Two additional variables, 'national' and 'state', were created using the existing 'state' and 'end date' variables. A short description of each variable of interest and their respective measurement methods is given below.

-   *Pollster*: The name of the polling organization conducting the poll (e.g., YouGov, RMG Research). This variable helps adjust for poll-specific biases. Every pollster that publicly published a scientific poll about the 2024 U.S. Presidential Election is included in the data set. The value measured for this variable is the name that appears in the scientific poll [@538_methodology].

-   *Numeric Grade*: A numeric rating given to the pollster, representing the accuracy and methodological transparency of the organization (e.g., 3.0), with higher grades indicating more reliable pollsters. The value of this variable is calculated by the 538 team. All national and state-level polls that were conducted in 1998 or later are considered to determine and assign a numeric grade to each pollster [@538_morris].

-   *Pollscore*: A score that reflects the reliability of each pollster, capturing their historical accuracy and biases. Negative values indicate better predictive accuracy, and rewards pollsters who are accurate and precise. This variable is calculated by aggregating predictive error and predictive bias together [@538_morris]. Predictive error is the difference between expectations and reality [@def_pred_error] and predictive bias is the systematic bias introduced in the methodology [@def_pred_bias].

-   *State*: The U.S. state where the poll was conducted, allowing for analysis of regional differences in candidate support. The value measured for this variable is the primary state that appears in the scientific poll's methodology [@538_methodology].

-   *National*: A binary variable indicating whether the poll is national (1 for national polls, 0 for state polls).

-   *End Date*: The date the poll ended, reflecting the currency of the data. This variable is taken from the publication and is the last date for which the survey was active [@538_methodology].

-   *Sample Size*: The total number of respondents in the poll. This is measured by the number of completed surveys [@538_methodology].

-   *Candidate Name*: The name of the candidate being polled (e.g., Kamala Harris or Donald Trump), identifying the focus of each poll result.

-   *Pct (Percentage)*: The percentage of respondents in the poll who support the specified candidate.

-   *Recency*: This variable measures how recent each poll is. This variable is calculated by subtracting the end date of the poll from the end date of the most recent poll.

\newpage

```{r}
#| warning: false
#| message: false
#| echo: false
#| label: tbl-sumstat
#| tbl-cap: "Summary Statistics of Numerical Variable"
analysis_data |> 
  select(numeric_grade, pollscore, sample_size, pct, recency) |> 
  datasummary_skim(histogram = FALSE)
```

\vspace{0.7cm}

@tbl-sumstat shows the average poll in our data set has a numeric grade of 2.2. This suggests most polls are of moderate to high quality in terms of reliability. The mean poll score of -0.5 suggests these polls are accurate, as negative values imply reduced bias. Also, the large average sample size of 1908 respondents across polls reduces variability and ensures reliable predictions for the candidate support model.

\newpage

### Variation of Poll Quality and Support for Candiates by Pollster

```{r}
#| warning: false
#| message: false
#| echo: false
#| label: tbl-topster
#| tbl-cap: "Top 5 most frequent pollsters, with count of polls, average pollscore (lower scores indicate less bias), and average numeric grade (higher values indicate greater reliability)."
# Get the top 10 pollsters by frequency along with their average pollscore and numeric grade
top_pollsters <- analysis_data %>%
  group_by(pollster) %>%           # Group by the 'pollster' variable
  summarize(
    Count = n(),                   # Count the number of occurrences
    Average_Pollscore = round(mean(pollscore, na.rm = TRUE), 2),  # Average pollscore
    Average_Numeric_Grade = round(mean(numeric_grade, na.rm = TRUE), 2),  # Average numeric grade
    .groups = 'drop'               # Drop grouping structure for cleaner output
  ) %>%
  arrange(desc(Count)) %>%         # Arrange in descending order of count
  slice_head(n = 5)                # Select the top 10 pollsters

# Rename the columns to remove underscores and capitalize 'Pollster'
colnames(top_pollsters) <- c("Pollster", "Count", "Average Pollscore", "Average Numeric Grade")

kable(top_pollsters, format = "markdown")

```

@tbl-topster shows the top 5 most frequent pollsters in the dataset, with each pollster’s total poll count, average poll score (measuring reliability and historical accuracy), and average numeric grade (indicating overall quality). The most frequent pollster, Morning Consult, has an average poll score and a slightly above-average numeric grade, suggesting the data provided is reliable. However, there is variability in scores and numeric grades across the top pollsters, showing differences in quality and potential biases among them.

```{r, fig.width = 12, fig.height = 6}
#| warning: false
#| message: false
#| echo: false
#| label: fig-ster
#| fig-cap: 'Support distribution for Kamala Harris and Donald Trump by major pollsters, highlighting variability in reported support across organizations and the value of aggregating multiple polls.'

# Filter the data for top 5 pollsters, if needed
top_pollsters <- analysis_data %>%
  count(pollster) %>%
  top_n(4, n) %>%
  pull(pollster)

filtered_data <- analysis_data %>%
  filter(pollster %in% top_pollsters)

# Plot the density plots for each pollster with overlaid densities for each candidate
ggplot(filtered_data, aes(x = pct, fill = candidate_name)) +
  geom_density(alpha = 0.5, color = "black") +
  scale_fill_manual(values = c("Kamala Harris" = "steelblue", "Donald Trump" = "indianred")) +
  facet_wrap(~ pollster, ncol = 1) +
  labs(x = "Support (%)", y = "Density") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 0, hjust = 1),
        axis.text.y = element_text(size = 10),
        legend.title = element_blank(),
        legend.position = "right",
        panel.spacing = unit(0.5, "lines"))

```

@fig-ster shows the distribution of support for Kamala Harris and Donald Trump by pollsters, highlighting variability across different polling organizations. Morning Consult, for example, demonstrates a wide range of reported support, with more spread in Trump’s support. In contrast, Siena/NYT shows less variation, with Harris consistently leading. This variability across pollsters shows the importance of aggregating multiple polls to account for organization-specific biases and ensure a more balanced view of candidate support.

### Sample Size of Polls

\vspace{0.7cm}

```{r, fig.width = 12, fig.height = 6}
#| warning: false
#| message: false
#| echo: false
#| label: fig-ss
#| fig-cap: "Distribution of Sample Sizes Across Polls: Majority of polls have sample sizes under 5,000, with a few outliers at larger sizes."

# Plot histogram for all sample sizes
ggplot(analysis_data, aes(x = sample_size)) +
  geom_histogram(binwidth = 500, fill = "lightblue", color = "black", alpha = 0.7) +
  labs(
       x = "Sample Size",
       y = "Count") +
  theme_minimal() +
  theme(
    panel.grid = element_blank(),  # Remove grid lines
    plot.title = element_text(size = 14, hjust = 0.5),  # Center the title
    axis.title.y = element_text(margin = margin(r = 10))  # Increased gap for y-axis label
  )

```

@fig-ss shows a clear right-skewed distribution. Most of the sample sizes are clustered between 0 and 3000 respondents, with a sharp peak around 1000-1500 respondents. This indicates that the majority of polls have smaller sample sizes. As sample size increases, the frequency decreases, with few polls conducted with sample sizes larger than 5000, though there are a few outliers with sizes approaching 10,000 or more. This wide range in sample sizes can affect the precision of estimates across different polls.

### Distribution of Numeric Grade and Pollscore of Polls

```{r, fig.width = 12, fig.height = 6}
#| warning: false
#| message: false
#| echo: false
#| label: fig-reli
#| fig-cap: "Distribution of Numeric Grade and Pollscore among polling organizations, highlighting variability in pollster reliability and potential bias across polls."
# Histogram with density line for numeric_grade
density_plot <- ggplot(analysis_data, aes(x = numeric_grade)) +
  geom_histogram(aes(y = after_stat(density)), binwidth = 0.1, fill = "lightblue", color = "black", alpha = 0.6) +
  geom_density(color = "black", size = 0.7) +
  labs(x = "Numeric Grade", y = "Density") +
  theme_minimal()

# Histogram with density line for pollscore with more x-axis labels
bar_plot <- ggplot(analysis_data, aes(x = pollscore)) +
  geom_histogram(aes(y = after_stat(density)), binwidth = 0.2, fill = "lightblue", color = "black", alpha = 0.6) +
  geom_density(color = "black", size = 0.7) +
  labs(x = "Pollscore", y = "Density") +
  scale_x_continuous(breaks = seq(-1.5, 1.5, by = 0.5)) +  # Adds more frequent x-axis labels
  theme_minimal() +
  theme(legend.position = "none")

# Combine the plots side by side with a wider layout
combined_plot <- density_plot + bar_plot + plot_layout(ncol = 2, widths = c(1.2, 1.2))

# Display the combined plot
combined_plot

```

@fig-reli shows the distribution of Numeric Grade (left) and Pollscore (right) across polling organizations. The numerical grade distribution shows that most pollsters are rated between 1.5 and 3, with peaks around 2.0 and 2.5, suggesting a concentration of pollsters with moderate to high reliability scores. In contrast, the Pollscore distribution, where lower values indicate higher reliability, shows a range primarily between -1.5 and 0, with a notable peak around -0.5. This suggests that while many polls demonstrate relatively low bias, there is still variability in reliability across organizations. The distinction between these two metrics emphasizes the need to consider both quality (Numeric Grade) and potential systematic bias (Pollscore) when weighing polls in the model.

### Distribution of Polls by Poll Type and Candidate

@fig-pie illustrates the distribution of polls between candidates and poll types. The left chart shows that the majority of polls are conducted at the state level, with a smaller portion being national. The right chart shows that polling is almost evenly split between Kamala Harris and Donald Trump. This distribution underscores the model’s balanced approach to capturing state-level differences as well as broader national trends, providing a view of candidate support across different contexts.

```{r, fig.width = 12, fig.height = 4}
#| warning: false
#| message: false
#| echo: false
#| label: fig-pie
#| fig-cap: "Poll distribution by type (state vs. national) and candidate (Trump vs. Harris), showing a majority of state polls and near-equal coverage for each candidate."
# Prepare data for National vs. State pie chart
national_state_data <- analysis_data %>%
  count(national) %>%
  mutate(label = ifelse(national == 1, "National", "State"))

# Pie chart for National vs. State
national_state_pie <- ggplot(national_state_data, aes(x = "", y = n, fill = label)) +
  geom_col(color = "darkgrey") +
  coord_polar(theta = "y") +
  scale_fill_manual(values = c("State" = "darkgrey", "National" = "white")) +
  labs(fill = "Poll Type") +
  theme_void() +
  theme(legend.position = "bottom")

# Prepare data for Kamala vs. Trump pie chart
candidate_data <- analysis_data %>%
  filter(candidate_name %in% c("Kamala Harris", "Donald Trump")) %>%
  count(candidate_name)

# Pie chart for Kamala vs. Trump
candidate_pie <- ggplot(candidate_data, aes(x = "", y = n, fill = candidate_name)) +
  geom_col(color = "white") +
  coord_polar(theta = "y") +
  scale_fill_manual(values = c("Kamala Harris" = "steelblue", "Donald Trump" = "indianred")) +
  labs( fill = "Candidate") +
  theme_void() +
  theme(legend.position = "bottom")

# Combine the two pie charts side by side
combined_plot <- national_state_pie + candidate_pie + plot_layout(ncol = 2)

# Display the combined plot
combined_plot

```

\newpage

### Support Trend For Candidates

```{r, fig.width = 12, fig.height = 6}
#| warning: false
#| message: false
#| echo: false
#| label: fig-trend
#| fig-cap: "Support trends for Kamala Harris and Donald Trump over time. Trend lines indicate slight shifts in support as the election nears, with consistent polling frequency throughout the period."

# Sample data creation (Replace this with your actual data)
# Assuming your dataset `analysis_data` has columns: date, pct, and candidate_name

# Filtering for only the necessary candidates if required
filtered_data <- analysis_data %>%
  filter(candidate_name %in% c("Kamala Harris", "Donald Trump"))

# Plotting
ggplot(filtered_data, aes(x = as.Date(end_date), y = pct, color = candidate_name)) +
  geom_point(alpha = 0.3, size = 1.5) +
  geom_smooth(method = "loess", se = TRUE, span = 0.2) +
  scale_color_manual(values = c("Kamala Harris" = "steelblue", "Donald Trump" = "indianred")) +
  labs(x = "Date", y = "Support (%)") +
  theme_minimal() +
  theme(
    legend.position = "right",
    legend.title = element_blank(),
    axis.text.x = element_text(angle = 0),
    axis.title.x = element_text(margin = margin(t = 10)),
    axis.title.y = element_text(margin = margin(r = 10))
  )

```

@fig-trend shows the support trends for Kamala Harris and Donald Trump from August to October. Each point represents a poll result, color-coded by candidates, with the trend lines highlighting the overall changes in support over time. Kamala Harris's support remains relatively steady but shows slight fluctuations around mid-September, while Donald Trump's support appears to have a small upward trend toward October. The distribution of points is dense throughout, reflecting consistent polling activity, though some dates show more concentrated polling. This visualization indicates that while both candidates maintain stable support levels, minor shifts occur as the election approaches, underscoring the importance of tracking trends over time rather than relying on individual polls.

\newpage

### Relationship Between Sample Size and Suppoert for Candidates

```{r, fig.width = 12, fig.height = 6}
#| echo: false
#| warning: false
#| message: false
#| label: fig-pct
#| fig-cap: "Relationship between Sample Size and Support Percentage for Donald Trump and Kamala Harris, showing slight downward and upward trends in support with increasing sample size for Trump and Harris, respectively."

# Filter data for both candidates and cap sample size at 12,000
filtered_data <- analysis_data %>%
  filter(sample_size <= 12000, candidate_name %in% c("Kamala Harris", "Donald Trump"))

# Create the plot for both Kamala Harris and Donald Trump
ggplot(filtered_data, aes(x = sample_size, y = pct, color = candidate_name)) +
  geom_point(size = 0.5, alpha = 0.7) +  # Smaller dots with slight transparency
  geom_smooth(aes(group = candidate_name), method = "lm", se = FALSE, color = "darkgrey", size = 0.8) +  # LOESS smoothing line
  
  # Custom colors for candidate
  scale_color_manual(values = c("Kamala Harris" = "steelblue", "Donald Trump" = "indianred")) +
  
  # Facet wrap by candidate name to get two plots
  facet_wrap(~ candidate_name, scales = "free_y") +
  
  # Minimal theme adjustments
  theme_minimal() +
  theme(
    panel.grid.major = element_blank(),  # Remove major gridlines
    panel.grid.minor = element_blank(),  # Remove minor gridlines
    legend.position = "none",  # Remove legend as colors are self-explanatory
    plot.title = element_text(hjust = 0.5),  # Center the title
    axis.line = element_line(color = "black"),  # Add axis lines
    panel.border = element_blank(),  # Remove panel border
    axis.line.y.right = element_line(color = "black"),  # Add right spine
    axis.line.x.top = element_blank()  # Remove top spine
  ) +
  labs(x = "Sample Size", y = "Support (%)")  # Title and axis labels

```

@fig-pct shows the relationship between sample size and support percentage for Donald Trump and Kamala Harris. For both candidates, the majority of polls have a sample size below 2,500, but there are some larger polls exceeding 10,000 respondents. In Trump’s chart, there’s a slight downward trend, suggesting that larger sample sizes may show marginally lower support. In contrast, Harris's chart shows a slight upward trend with larger sample sizes, indicating a marginal increase in support with larger poll samples. This highlights the variability in polling support depending on the sample size, emphasizing the importance of including sample size in our model.

### Variability of Candidate support Across US States

```{r}
#| echo: false
#| warning: false
#| message: false
#| inlcude: false

# Create the dataset for mapping support
support_by_state <- analysis_data %>%
  # Filter for only Kamala Harris and Donald Trump
  filter(candidate_name %in% c("Kamala Harris", "Donald Trump")) %>%
  # Group by state and candidate
  group_by(state, candidate_name) %>%
  # Calculate the mean support percentage for each candidate in each state
  summarize(mean_support = mean(pct, na.rm = TRUE)) %>%
  # Spread the data so that each state has a column for each candidate's mean support
  pivot_wider(names_from = candidate_name, values_from = mean_support) %>%
  # Rename columns for clarity
  rename(kamala_mean_support = `Kamala Harris`, trump_mean_support = `Donald Trump`) %>%
  # Calculate total mean support for both candidates in each state
  mutate(total_support = kamala_mean_support + trump_mean_support,
         # Calculate proportion of support for each candidate out of the total
         kamala_proportion = kamala_mean_support / total_support,
         trump_proportion = trump_mean_support / total_support) %>%
# Select relevant columns
  select(state, kamala_proportion, trump_proportion)

```

```{r}
#| warning: false
#| message: false
#| echo: false
#| include: false

map_data <- us_states %>%
  left_join(support_by_state, by = c("NAME" = "state")) 
```

```{r, fig.width = 12, fig.height = 6}
#| echo: false
#| warning: false
#| message: false
#| label: fig-map1
#| fig-cap: "Proportion of support for Kamala Harris relative to Donald Trump across U.S. states. Blue indicates states where Harris has higher support, while red indicates states where Trump leads. Color intensity reflects the magnitude of support difference, with gray indicating insufficient polling data."


# Filter out Alaska, Hawaii, and Puerto Rico using subset
map_data_continental <-
  subset(map_data,!NAME %in% c("Alaska", "Hawaii", "Puerto Rico"))

ggplot(map_data_continental) +
  geom_sf(aes(fill = kamala_proportion * 100),
          color = "white",
          size = 0.2) +
  geom_text(
    aes(label = STUSPS, geometry = geometry),
    stat = "sf_coordinates",
    color = "black",
    size = 3
  ) +  # Adjust color and size as needed
  scale_fill_distiller(
    palette = "RdBu",
    direction = 1,
    na.value = "grey",
    name = "Harris's Mean Support in Polls (%)"
  ) +
  theme_minimal() +
  theme(
    panel.grid = element_blank(),
    axis.text = element_blank(),
    axis.title = element_blank(),
    axis.ticks = element_blank(),
    aspect.ratio = 0.5  # Make the plot wider
  )
```

@fig-map1 shows the average support for Kamala Harris as a proportion relative to both Kamala Harris and Donald Trump across the continental U.S. states. States shaded in blue indicate a higher proportion of support for Harris, while red shades represent stronger support for Trump, with color intensity reflecting the support margin. Gray-shaded states lack sufficient polling data.

In swing states, such as Pennsylvania, Michigan, and Arizona, there is a balanced distribution of support, suggesting close competition in these battleground areas. Meanwhile, traditional Democratic strongholds, like California and New York, show a preference for Harris, with deeper blue shades. Conversely, traditionally Republican states, such as Texas and Tennessee, exhibit higher support for Trump, with intense red shades highlighting his advantage. This visualization emphasizes the varied regional dynamics of candidate support, with distinct patterns in both competitive and historically partisan states.

# Forecasting Election Outcome through Pooling Polls {#sec-model}

## Forecasting Approach

The polls of polls methodology is widely used in election prediction as it aggregates multiple polls to provide a more reliable estimate of voter support, rather than relying on any single poll. The goal is to reduce errors and biases present in individual polls by using a weighted average of many different polls.

In our approach, we will employ linear modeling of voter support percentage (pct) on pollster and other independent variables such as sample size, poll recency, and poll scope (state vs. national). This will allow us to smooth out the inherent noise, biases, and variability across different pollsters. Once we obtain the predicted values from our model, we will weight these predictions based on the numeric grade (quality score) of each pollster to calculate an overall national estimate of the outcome. Additionally, we will separately compute estimates for key battleground states, where voter behavior can be more volatile and pivotal in deciding the outcome of the election. This approach helps us capture both national trends and state-level dynamics.

## Model

In this section, we aim to address the inherent biases and differences present in various polling data to arrive at a robust prediction model. The core challenge lies in selecting a model with an optimal balance between complexity and fit, ensuring it accurately captures the dynamics of polling data while avoiding overfitting. To this end, we carefully evaluated different model specifications to determine the most appropriate one for our forecasting purpose.

Given that variables like numeric grade and poll score are perfectly collinear with the pollster, they were excluded from the regression analysis to avoid multicollinearity issues. These variables, however, remain integral to our weighting strategy, where they will be used to adjust for differences in polling accuracy and reliability. Instead, we focus on key features such as pollster, sample size, state, and recency, gradually adding complexity to the model.

By comparing model specifications that incorporate these variables, we aim to select the model with the right balance between predictive accuracy and generalizability, ultimately providing the best possible forecast.

## Model Set Up

We aim to model the percentage of support for Kamala Harris and Donald Trump in each poll as a function of the pollster the sample size, the state, and the recency of the poll.

$$
y_i = \alpha + \beta_1 \cdot \mathrm{pollster}_i + \beta_2 \cdot \mathrm{sample\_size}_i + \beta_3 \cdot \mathrm{recency}_i +  \beta_4 \cdot \mathrm{state}_i + \epsilon_i
$$

Where

-   $y_i$ is the percentage of support for candidate in poll i,

-   $α$ is the intercept,

-   $β_1$ captures the effect of the polling organization,

-   $β_2$ captures the effect of the sample size,

-   $β_3$ captures the effect of recency (how recent the poll is),

-   $β_4$ capture the effects of the different states

-   $\epsilon_i$ represents the error term, assumed to follow a normal distribution with mean 0.

## Model Justification

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: fig-causal-model
#| fig-cap: "Factors influencing candidate support, with poll attributes (Pollster, Sample Size, State, Recency) as direct contributors and unobserved confounders representing potential biases in polling results."

# Load and display the saved PNG
knitr::include_graphics(here("other", "graphs", "polls_of_polls_dag.png"))

```

In our model, we aim to smooth out discrepancies and biases across various polling organizations using a polls-of-polls approach. Given the potential for individual pollsters to introduce systematic differences—due to variations in sampling methods, question phrasing, and historical leanings—our model includes a pollster variable to adjust for these organization-specific biases. This allows us to capture an aggregated view of public support that is less susceptible to the idiosyncrasies of any single poll. Furthermore, we incorporate sample size as a predictor, as polls with larger samples tend to yield more stable and reliable results, reducing random fluctuations caused by smaller samples. The state variable accounts for regional political differences, ensuring that the model captures varying levels of support across geographic and demographic groups, which is important for understanding the political landscape. Additionally, recency is included to prioritize more recent polls, as public opinion can shift in response to political events, and recent data is generally more reflective of current sentiment. By integrating these factors, our model seeks to produce a more stable measure of candidate support, reducing noise from individual poll discrepancies and focusing on a balanced, up-to-date aggregation of polling data.

We opted for a linear model due to its capacity to quantify the marginal effects of each predictor (pollster, sample size, state, and recency) on candidate support. This structure is well-suited to our polls-of-polls approach, as it allows for the estimation of fixed effects that can control for systematic biases across pollsters, while accommodating the influence of sample size and recency as continuous variables. All modeling was conducted using the base R package [@citeR], specifically utilizing the lm() function from the stats package for linear regression analysis.

## Model Results

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: tbl-summary
#| tbl-cap: "Model performance summary showing improved fit and accuracy as State and Recency are added, with R² increasing from 0.375 in Model 1 to 0.774 in Model 3, and RMSE decreasing from 3.053 to 1.836."


# Load your models
model1 <- readRDS(here("models/model1.rds"))
model2 <- readRDS(here("models/model2.rds"))
model3 <- readRDS(here("models/model3.rds"))

# Extract summary statistics and add variables included in each model
model_summary <- tibble::tibble(
  Model = c("Model 1", "Model 2", "Model 3"),
  Variables = c(
    "Pollster, Sample Size",
    "Pollster, Sample Size, State",
    "Pollster, Sample Size, State, Recency"
  ),
  `R²` = c(
    summary(model1)$r.squared,
    summary(model2)$r.squared,
    summary(model3)$r.squared
  ),
  `Adjusted R²` = c(
    summary(model1)$adj.r.squared,
    summary(model2)$adj.r.squared,
    summary(model3)$adj.r.squared
  ),
  `AIC` = round(c(AIC(model1), AIC(model2), AIC(model3)), 0),
  `BIC` = round(c(BIC(model1), BIC(model2), BIC(model3)), 0),
  `RMSE` = c(sqrt(mean(
    residuals(model1) ^ 2
  )), sqrt(mean(
    residuals(model2) ^ 2
  )), sqrt(mean(
    residuals(model3) ^ 2
  )))
)

# Display the table using kable
model_summary %>%
  kable(caption = "Model Summary with Included Variables", digits = 3)

```

@tbl-summary summarizes the performance metrics for three models with progressively added variables. Model 1, which includes only Pollster and Sample Size, achieves an R² of 0.375, indicating that these variables alone explain about 37.5% of the variance in candidate support. Model 2 incorporates State as an additional predictor, resulting in a substantial improvement, with an R² of 0.720 and a reduction in both AIC and RMSE, showing better model fit and predictive accuracy. Model 3 further adds Recency, which increases the R² to 0.774 and decreases the RMSE to 1.836, indicating enhanced explanatory power and prediction accuracy. This progression highlights the benefit of adding contextual variables like State and Recency to better capture the complexities of polling data.[@fig-modsum1] helps us the visualise the model results.

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: fig-modsum1
#| fig-cap: 'Model summaries for Kamala Harris and Donald Trump at the national level using polling data.'

# Load and display the saved PNG
knitr::include_graphics(here("other", "graphs", "sum1.png"))

```

## Prediction

To predict Kamala Harris' overall support, we used a weighted average approach based on the quality of each poll. The weights are calculated using each poll's `numeric_grade`, which reflects the reliability and transparency of the polling methodology.

We define the weight for each pollster $w_i$ as follows:

$$
w_i = \frac{\mathrm{numeric\_grade}_i \times (\mathrm{maxPollscore} - \mathrm{pollscore}_i)}{\sum_{i=1}^{n} \mathrm{numeric\_grade}_i \times (\mathrm{maxPollscore} - \mathrm{pollscore}_i)}
$$

where:

-   $w_i$ represents the weight assigned to poll i,

-   $numericgrade_i$ is the numeric grade of poll i, and

-   $n$ is the total number of polls used in the analysis.

-   $pollscore_i$ is the is the pollscore of poll i which reflects the estimated bias of the poll (with more negative values indicating less bias)

-   $maxPollscore$ is the maximum pollscore across all polls.

The weight assigned to each poll combines both its quality (as represented by the numeric grade) and its level of bias (as indicated by the poll score). Since a more negative poll score reflects a lower level of bias, the formula uses the difference between the maximum poll score and each poll’s specific poll score. This approach gives more weight to polls that are both highly graded (indicating higher reliability and transparency) and less biased. By combining these two factors, the weighting system emphasizes polls that are reliable and minimally biased, ensuring that they have a stronger influence on the overall calculation. Additionally, the total of all weights is normalized to sum to one, so each poll’s weight is proportionate to its quality and relative lack of bias, resulting in a more balanced and accurate average of public support.

Using these weights, the overall weighted prediction of candidate's support is calculated by summing the weighted predicted values from our regression model:

$$
\text{Overall Weighted Support} = \sum_{i=1}^{n} w_i \cdot \hat{y}_i
$$

-   $\hat{y}_i$ is the predicted percentage of support for Kamala Harris from poll i.

### Comparing Overall Weighted Support Across All Polls

Aggregating support across all polls and applying our weighted approach, we estimate the overall support for each candidate. The weights, calculated based on both the poll's numeric grade and poll score, ensure that higher-quality polls with less bias contribute more to our estimates. Based on this approach, Kamala Harris has an estimated overall weighted support of around 47.77%, while Donald Trump stands at around 46.31%. This aggregation takes into account the varied methodologies and sampling qualities of different polling organizations.

```{r}
#| warning: false
#| message: false
#| echo: false
#| include: true

model6 <- readRDS(here("models/model6.rds"))

# Step 1: Subset data for Kamala Harris, calculate weights, join with fitted values from model3, and calculate weighted_pct
kamala_data <- analysis_data %>%
  filter(candidate_name == "Kamala Harris") %>%
  mutate(
    weight = numeric_grade * (max(analysis_data$pollscore, na.rm = TRUE) - pollscore) /
      sum(numeric_grade * (
        max(analysis_data$pollscore, na.rm = TRUE) - pollscore
      ), na.rm = TRUE),
    fitted_values = fitted(model3),
    # Replace this with actual fitted values for Kamala
    weighted_pct = fitted_values * weight  # Calculate weighted percentage for Kamala
  )

# Calculate the overall weighted prediction for Kamala Harris
kamala_weighted_support <-
  sum(kamala_data$weighted_pct, na.rm = TRUE)

# Step 2: Subset data for Donald Trump, calculate weights, join with fitted values from model6, and calculate weighted_pct
trump_data <- analysis_data %>%
  filter(candidate_name == "Donald Trump") %>%
  mutate(
    weight = numeric_grade * (max(analysis_data$pollscore, na.rm = TRUE) - pollscore) /
      sum(numeric_grade * (
        max(analysis_data$pollscore, na.rm = TRUE) - pollscore
      ), na.rm = TRUE),
    fitted_values = fitted(model6),
    # Replace this with actual fitted values for Trump
    weighted_pct = fitted_values * weight  # Calculate weighted percentage for Trump
  )

# Calculate the overall weighted prediction for Donald Trump
trump_weighted_support <- sum(trump_data$weighted_pct, na.rm = TRUE)
```

### State-Level Predictions

In this section, we examine the predicted support for Kamala Harris and Donald Trump within each state, based on our weighted approach. By aggregating poll results within each state and applying weights that adjust for poll quality and bias, we can estimate candidate support with a more localized perspective. This allows us to identify state-by-state competition and highlight key battleground states where the margins are close.

For each state, we compare the predicted support percentages and determine the projected winner based on which candidate has higher weighted support. We also calculate the margin of difference between the candidates, which shows how close each state’s race is.

```{r}
#| warning: false
#| message: false
#| echo: false
#| include: false
# Define the max pollscore across the dataset
max_pollscore <- max(analysis_data$pollscore, na.rm = TRUE)

# Step 1: Subset data for Kamala Harris, join fitted values, then calculate weights within each state
kamala_data_a2 <- analysis_data %>%
  filter(candidate_name == "Kamala Harris") %>%
  mutate(fitted_values = fitted(model3)) %>%  # Add fitted values from model 3 for Kamala Harris
  group_by(state) %>%
  mutate(weight = numeric_grade * (max_pollscore - pollscore) /
           sum(numeric_grade * (max_pollscore - pollscore), na.rm = TRUE)) %>%
  # Calculate the overall weighted prediction for each state
  summarize(support_kamala = sum(fitted_values * weight, na.rm = TRUE))

# Step 2: Repeat for Donald Trump with model6
trump_data_a2 <- analysis_data %>%
  filter(candidate_name == "Donald Trump") %>%
  mutate(fitted_values = fitted(model6)) %>%  # Add fitted values from model 6 for Donald Trump
  group_by(state) %>%
  mutate(weight = numeric_grade * (max_pollscore - pollscore) /
           sum(numeric_grade * (max_pollscore - pollscore), na.rm = TRUE)) %>%
  # Calculate the overall weighted prediction for each state
  summarize(support_trump = sum(fitted_values * weight, na.rm = TRUE))

# Step 3: Join the two datasets by state
state_support_data <-
  left_join(kamala_data_a2, trump_data_a2, by = "state")

# Add a new column 'winner' based on higher support values
state_support_data <- state_support_data %>%
  mutate(
    winner = ifelse(support_kamala > support_trump, "Kamala Harris", "Donald Trump"),
    margin = abs(support_kamala - support_trump)
  )
```

```{r, fig.width = 12, fig.height = 6}
#| echo: false
#| warning: false
#| message: false
#| label: fig-map2
#| fig-cap: "Predicted 2024 U.S. Presidential Election winner by state, with Kamala Harris-leaning states in blue, Donald Trump-leaning states in red, and states lacking sufficient polling data in gray. The map highlights traditional Democratic and Republican strongholds as well as key battleground states."

# Filter out Alaska, Hawaii, and Puerto Rico from state_support_data
map_data_continental <-
  subset(us_states,!NAME %in% c("Alaska", "Hawaii", "Puerto Rico"))

# Merge with state_support_data to add winner info
map_data_continental <- map_data_continental %>%
  left_join(state_support_data,  by = c("NAME" = "state"))

# Define the colors for each category
color_scale <-
  c(
    "Kamala Harris" = "steelblue",
    "Donald Trump" = "indianred",
    "No Polling Data" = "grey"
  )

# Plotting the map with ggplot2
ggplot(map_data_continental) +
  geom_sf(aes(fill = winner), color = "white", size = 0.2) +
  geom_text(
    aes(label = STUSPS, geometry = geometry),
    stat = "sf_coordinates",
    color = "black",
    size = 3
  ) +  # Adjust text size as needed
  scale_fill_manual(
    values = color_scale,
    na.value = "grey",
    name = "2024 Election Winner",
    labels = c("Kamala Harris", "Donald Trump", "No Polling Data")
  ) +
  theme_minimal() +
  theme(
    panel.grid = element_blank(),
    axis.text = element_blank(),
    axis.title = element_blank(),
    axis.ticks = element_blank(),
    aspect.ratio = 0.5  # Adjust the aspect ratio to make the map wider
  ) 
```

@fig-map2 presents the predicted winner of the 2024 U.S. Presidential Election by state based on aggregated polling data, with red representing states projected to favor Kamala Harris and blue indicating those favoring Donald Trump. Notably, traditional Democratic strongholds in the Northeast and West Coast, such as California, New York, and Washington, show solid support for Harris. Conversely, traditional Republican states, including Texas, Florida, and much of the South, show strong support for Trump.

Swing states, like Pennsylvania, Michigan, and Wisconsin, are predominantly leaning toward Harris, indicating a possible advantage for her in these key battlegrounds, although other typical swing states like Arizona and Georgia lean toward Trump. Gray-shaded states lack sufficient polling data to make a prediction, reflecting areas of data scarcity in the model's projections. This visualization highlights the geographical and political divides across the U.S., as well as the importance of swing states in election outcomes.

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: tbl-battleground-states
#| tbl-cap: "Predicted support for Kamala Harris and Donald Trump across key battleground states, indicating the winner and the support margin (%) in each state."

# Define battleground states based on provided list
battleground_states <-
  c(
    "Pennsylvania",
    "Michigan",
    "North Carolina",
    "Wisconsin",
    "Nevada",
    "Arizona",
    "Georgia"
  )

# Display the table of battleground states with adjusted column names
battleground_table <- state_support_data %>%
  filter(state %in% battleground_states) %>%
  select(
    State = state,
    `Kamala Support (%)` = support_kamala,
    `Trump Support (%)` = support_trump,
    `Predicted Winner` = winner,
    `Support Margin (%)` = margin
  ) %>%
  mutate(
    `Kamala Support (%)` = round(`Kamala Support (%)`, 2),
    `Trump Support (%)` = round(`Trump Support (%)`, 2),
    `Support Margin (%)` = round(`Support Margin (%)`, 2)
  )

# Display the table using kable
knitr::kable(battleground_table)
```

Many polling websites such as USAFacts [@usafacts] and FiveThirtyEight [@FiveThirtyEight] are calling Arizona, Georgia, Michigan, Pennsylvania, Wisconsin, North Carolina and Nevada the 'swing states' of the 2024 presidential election. Swing states are important for election predictions because these states, with their historically close voting margins, often determine the overall outcome by tipping the electoral balance toward one candidate. @tbl-battleground-states summarizes the predicted support percentages for Kamala Harris and Donald Trump in key battleground states, along with the predicted winner and the support margin. The data shows close margins in these states, with some like Arizona and Georgia leaning towards Trump, while others such as Michigan and Pennsylvania favor Harris by narrow margins. The support margin column highlights the competitiveness of these states, with all margins below 2.2%.

# Discussion {#sec-discussion}

## Key Findings

This paper predicts the support for Kamala Harris and Donald Trump in the 2024 U.S. Presidential Election using a “poll-of-polls” approach. By aggregating multiple polls, we reduce the influence of individual surveys' biases and create a more accurate and balanced forecast across candidates. Our model uses predictors, such as pollster, sample size, state, and recency of the polls. We applied a weighting scheme based on each pollster’s numeric grade and poll score which takes into consideration reliability and bias. This weighted approach produces a prediction of candidate support that reflects variations across different states and polling organizations.

One main finding from our analysis is the importance of poll recency. Including recency notably increased the model’s explanatory power, with improvements in R² and reductions in RMSE compared to models without it. This underscores the dynamic nature of public opinion, where recent events can influence voter perceptions and candidate support.

Our state-level analysis highlights close competition observed in key battleground states, emphasizing their pivotal role in the 2024 election. Our model shows that in states like Pennsylvania, Michigan, and Wisconsin, support levels for Harris and Trump are nearly tied, reflecting the intense contest for these electoral votes. The predictions show that even minor shifts in support within these swing states could decisively impact the overall election outcome. This analysis shows the importance of regional polling in capturing voter sentiment and the heightened influence that battleground states hold in shaping the final results.

## Polling Uncertainty in 2024 Election

Polling provides a general sense of public opinion, but it shouldn’t be viewed as a guaranteed predictor of election outcomes. Past U.S. elections show why polls can be misleading. For example, in 2016, polls showed Hillary Clinton in the lead. According to national polls, Clinton would have won the popular vote by about 3.2% [@durand]. However, Donald Trump won battleground states, surprising many who relied on those predictions [@silver]. Trump won the electoral vote with 306 votes, while Clinton had 232 [@durand].

In 2020, 93% of national polls overestimated support for President Joe Biden [@kennedy]. Biden's lead was especially overestimated in states like Florida and Wisconsin, where Trump performed better than expected [@edwards]. These examples highlight the limitations of polls, especially when they can't capture late shifts in voter preferences, changes in turnout, and the difficulty of reaching a representative sample [@pew]. While polls can show trends, they are not always reliable for predicting what will happen on election day.
## Trump and Harris' Path to Victory

Both Donald Trump and Kamala Harris have potential paths to victory in the 2024 election. For Trump, one factor is the “shy voter” effect, where some supporters may be hesitant to share their views with pollsters due to social pressures. This can lead to polls underestimating his actual support [@woodie]. Additionally, Trump’s base includes groups that are harder to reach through standard polling, such as rural voters and individuals who avoid mainstream media [@brown]. These voters may be underrepresented in polls but can play an important role in election outcomes.

On the other hand, Kamala Harris could secure a win if her campaign mobilizes specific demographics, including young people, women, and minority voters. As the first Black and South Asian woman on a presidential ticket, Harris’s candidacy may energize these groups and boost turnout among historically Democratic-leaning voters [@pew]. The growing diversity and youth of the electorate could also benefit her, especially in urban areas and battleground states [@blow]. With strong efforts in voter registration and mobilization, Harris could turn polling support into actual votes, providing her with a strong chance at victory if turnout aligns with her campaign’s goals.

## Weaknesses and Future Directions

Our model assumes linear relationships between predictors and candidate support, which might not capture more complex or interaction-based trends. Additionally, while we weighted polls based on their numeric grades, these scores may not fully reflect each pollster’s accuracy, leaving room for improvement. Future research could explore non-linear methods, such as machine learning, to capture potential interactions between variables. Refining the weighting mechanism by considering pollster performance history or state-specific polling differences could further enhance predictive accuracy.

In conclusion, this paper contributes to election forecasting by providing a weighted estimate of candidate support across both national and state levels. The evolving nature of voter sentiment and poll accuracy suggests the need for adaptive models that take into consideration ongoing changes in public opinion.

\newpage

\appendix

# Appendix {.unnumbered}

# Data Cleaning and Modeling {#sec-data-model}

## Data Cleaning

The raw data for this project, sourced from FiveThirtyEight, [@FiveThirtyEight] underwent a series of cleaning steps to prepare it for analysis. Initially, duplicate rows were removed to ensure that unique observations remained, facilitated by the `janitor` package [@janitor]. A new binary variable, 'national', was created to indicate whether a poll was conducted at the national or state level. Missing values in the 'state' column were replaced with "Not Applicable," and numeric grades were evaluated to filter out low-quality pollsters, keeping only those with a numeric grade above 1. This cutoff was selected to retain mid to high-level pollsters for more reliable results. These steps were performed using functions from the `tidyverse` package [@tidy]. Dates were also standardized and converted into a proper format for analysis using the same package. Polls related to Kamala Harris were retained for further analysis, and percentage support values were transformed into actual numbers of supporters based on sample size. Pollster counts below five were excluded to focus on more reliable data sources. Polls regarding Kamala Harris were filtered to include only those conducted after her official candidacy announcement on July 21, 2024, ensuring the data reflects post-announcement public sentiment. The cleaned dataset was saved in Parquet format for efficient storage and retrieval, using the `arrow` package [@arrow].


## Model Diagnostics

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: fig-dgplotone
#| fig-cap: 'Diagnostic plots for Model 3'

knitr::include_graphics(here("other", "graphs", "diagnostic_plot_model3.png"))
```

The diagnostic plots for Model 3 are given in [@fig-dgplotone]. We notice the following:

1.  **Residuals vs. Fitted Plot**: This plot checks for non-linearity and heteroscedasticity. The residuals are scattered around the horizontal axis with no clear pattern, indicating that the linearity assumption is reasonable. However, there is a slight spread around the center, suggesting some mild heteroscedasticity.

2.  **Normal Q-Q Plot**: This plot assesses the normality of residuals. Most points align along the diagonal line, although there are some deviations at the tails. This suggests that the residuals are approximately normally distributed, with minor deviations in the extremes.

3.  **Scale-Location Plot**: This plot further checks for homoscedasticity. The residuals appear to be evenly spread across the fitted values, supporting the homoscedasticity assumption, although slight deviations are present in certain regions.

4.  **Residuals vs. Leverage Plot**: This plot identifies potential influential points. While most points have low leverage, a few points exhibit higher leverage, as indicated by their distance from the center. However, no points exceed Cook’s distance threshold, indicating no extreme outliers that would unduly influence the model.

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: fig-dgplottwo
#| fig-cap: 'Diagnostic plots for Model 6'

knitr::include_graphics(here("other", "graphs", "diagnostic_plot_model6.png"))
```

The diagnostic plots for Model 6 are given in [@fig-dgplottwo]. We notice the following:

1.  **Residuals vs Fitted**: This plot suggests a fairly random spread of residuals around zero, indicating that the linearity assumption holds reasonably well. However, there is a slight clustering of points in the center, which could indicate minor heteroscedasticity but is not severe.

2.  **Normal Q-Q Plot**: The Q-Q plot shows that the residuals generally follow a normal distribution, with only a few deviations at the tails. This suggests that the normality assumption is mostly met, though some minor deviations in the upper tail indicate possible outliers.

3.  **Scale-Location (Spread-Location) Plot**: The residuals appear randomly dispersed with no clear pattern, supporting the homoscedasticity (constant variance) assumption. However, some observations near the upper range might indicate slight variance inconsistency, though it is minimal.

4.  **Residuals vs Leverage**: This plot does not show any influential outliers with high leverage that might impact the model unduly. The Cook's distance lines show that no data points exceed these thresholds, indicating that no individual observation is disproportionately influencing the model.

Overall, the diagnostic plots for both models suggest that they meet the assumptions for linear regression fairly well, with minor deviations that are not expected to severely impact the model’s validity.

# Code Styling {#sec-styling}

The code in this paper was reviewed and formatted for consistency using lintr [@citelintr] and styler [@citestyler], ensuring readability and adherence to style standards.

# Reproducibility {#sec-reproducibility}

To replicate the findings presented in this paper, users should execute the scripts available in the GitHub repository. Begin by running the 00-install_packages.R script, which installs all required packages for the analysis.

# Acknowledgments {#sec-acknowledgement}

We extend our gratitude to [@citeTS], which provided invaluable guidance in establishing a reproducible workflow and inspired many of the code structures used in this paper.

\newpage

# References
